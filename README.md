# Image Captioning with BLIP

This project uses a deep learning model (BLIP: Bootstrapping Language-Image Pretraining) to generate **two types of captions** for an image:

-  A **short** summary-style caption
-  A **detailed** explanation-style caption

It then overlays both captions onto the image and saves:
-  The **annotated image**
-  A **captions.json** file with the text + confidence scores

---

##  Output Format

After running the script, you will find:

